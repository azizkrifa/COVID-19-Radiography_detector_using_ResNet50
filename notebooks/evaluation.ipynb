{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "import-header",
   "metadata": {},
   "source": [
    "# Model Evaluation Pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3450df",
   "metadata": {},
   "source": [
    "## üì¶ Imports for Evaluation\n",
    "\n",
    "- `tensorflow.keras.models.load_model`: Loads the trained model from a saved file.  \n",
    "- `utils.load_test_data`: Custom function to load the test dataset.  \n",
    "- `utils.visualize_samples`: Displays sample test images.  \n",
    "- `utils.classification_report_and_confusion_matrix`: Generates classification metrics (precision, recall, F1-score) and a confusion matrix.  \n",
    "- `utils.visualize_test_predictions`: Shows model predictions on test images for qualitative evaluation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e0f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from utils import load_test_data , visualize_samples , classification_report_and_confusion_matrix ,visualize_test_predictions\n",
    "import importlib\n",
    "import Shared_vars    \n",
    "importlib.reload(Shared_vars)  \n",
    "from Shared_vars import DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divider-1",
   "metadata": {},
   "source": [
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## üß™ Load Model and Test Data\n",
    "\n",
    "- `load_model`: Loads the best saved model (`best_model.h5`) from training.  \n",
    "- `load_test_data`: Loads the test dataset prepared earlier from `DATA_DIR`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007bfc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('/output/best_model.h5')\n",
    "test_data = load_test_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divider-2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-header",
   "metadata": {},
   "source": [
    "## üëÅÔ∏è Inspect Test Samples\n",
    "\n",
    "- `visualize_samples`: Displays a few random images from the test dataset along with their true class labels.  \n",
    "\n",
    "- This step helps confirm that the test data is correctly loaded and ready for evaluation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37df8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_samples(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divider-3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-header",
   "metadata": {},
   "source": [
    "## üìä Evaluate Model Performance\n",
    "\n",
    "- **Model Evaluation**:  \n",
    "  - `model.evaluate`: Computes test loss and accuracy on the test dataset.  \n",
    "\n",
    "- **Detailed Metrics**:  \n",
    "  - `classification_report_and_confusion_matrix`:  \n",
    "    - Generates precision, recall, and F1-score for each class.  \n",
    "    - Displays the confusion matrix to visualize correct vs. incorrect predictions.  \n",
    "\n",
    "This provides both overall performance metrics and class-level insights into model behavior.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5802dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_data)\n",
    "\n",
    "print(f\"\"\"‚úÖ Test Accuracy: {acc*100:.2f}%\n",
    "‚úÖ Test loss :{loss:.3f}\"\"\")\n",
    "\n",
    "classification_report_and_confusion_matrix(model , test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divider-4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predictions-header",
   "metadata": {},
   "source": [
    "## üéØ Visualize Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f82993",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_test_predictions(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
